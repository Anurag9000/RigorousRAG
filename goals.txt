High-level goals for extending the Academic Search Engine
=========================================================

Overall objective: Layer a modern RAG-style “search + internal knowledge + web” agent on top of the existing offline academic search engine, without removing any current features. The agent should be able to:
- Use the existing academic index as one source of truth.
- Use internal documents (e.g., a handbook) as another source.
- Use live web search (single URL and broader search with domain filters) as a third source.
- Synthesize everything into a structured answer with citations.


1. Single‑page URL ingestion tool
--------------------------------
- Add a reusable “get single web page” tool that:
  - Accepts a user-provided URL.
  - Fetches the page HTML (respecting timeouts and basic safety).
  - Converts the page into a clean text/markdown representation suitable for LLM context.
- Decide on implementation:
  - Preferred: use a library like `docling` (HTML → markdown) if available.
  - Fallback: reuse the existing `Crawler.Page` extraction logic (BeautifulSoup) and return plain text.
- Include minimal validation for the URL (scheme, domain, etc.).
- Return a structured payload (e.g., title, url, text/markdown, content_length) that downstream tools/agents can consume.


2. Broader web search tool (OpenAI web_search)
----------------------------------------------
- Add a tool that lets the agent perform broader web search via the OpenAI `web_search` tool:
  - Use the Responses API with the `web_search` tool definition.
  - Support domain filters (allowed_domains) derived from:
    - A configurable list of client-approved domains, and/or
    - Existing trusted domain lists (e.g., from `trusted_sources.py`).
- Return structured results:
  - An answer text.
  - A list of citations, each with `text` and `url` (and optionally `source_title`, `source_type`).
- Make tool choice flexible:
  - Allow the agent to decide when to use web search vs. other tools.
  - Support both “one-shot” search and iterative (multi-step) search flows where the model can perform several web_search calls.


3. Internal handbook / policy knowledge tool
--------------------------------------------
- Introduce a simple internal knowledge source based on one or more markdown files (e.g., `handbook.md`):
  - For now, assume the handbook is small enough to load in full into context.
  - In future, this can evolve into a full RAG pipeline (chunking + embeddings + retrieval).
- Create a tool function, e.g. `get_handbook()` / `search_handbook()` that:
  - Reads the markdown file(s) from disk.
  - Optionally supports simple keyword filtering or section extraction.
  - Returns the text as structured context for the model.
- Use Pydantic models for structured output:
  - `HandbookAnswer` with `answer: str` and `citations: List[HandbookCitation]`.
  - `HandbookCitation` with `text: str`, `section: Optional[str]`, `source: str` (e.g. filename or section anchor).
- Ensure the agent only calls this tool when it decides handbook context is useful (not for every question).


4. Integrate existing AcademicSearchEngine as a tool
----------------------------------------------------
- Expose the current offline academic index as an internal search tool:
  - Wrap `AcademicSearchEngine.search(...)` + `gather_context(...)` in a function that:
    - Accepts a natural-language query and result limit.
    - Returns a structured payload: list of hits + snippets + contexts.
  - Optionally create a Pydantic model, e.g. `InternalSearchResult` with:
    - `answer` (optional high-level summary).
    - `hits: List[SearchHit]` (reusing or mirroring the existing `SearchHit` dataclass).
    - `citations` generated from the hits (URL + title).
- Decide when this internal search is used:
  - As a default/primary source for academic questions.
  - As one of several tools that the agent can choose from.
- Preserve all existing non-agent workflows:
  - `Searching.py` should still work as a standalone CLI search tool.
  - `ai_search.py` single-source summarisation should remain functional.


5. Unified search-and-citations answer model
-------------------------------------------
- Define a single, consistent answer schema used across all tools:
  - Core model, e.g. `SearchAnswer` or `AgentAnswer` with:
    - `answer: str` (natural-language response).
    - `citations: List[Citation]`.
  - `Citation` model with:
    - `id: int` or `label: str` (e.g. “[1]”),
    - `title: str`,
    - `url: str`,
    - `source_type: Literal["internal_index", "handbook", "web_page", "web_search"]`,
    - Optional `snippet: str`.
- Align this with the existing `CitationSummary` in `llm_agent.py`:
  - Refactor or extend as needed so all paths (OpenAI, Ollama, extractive fallback, new tools) return the same shape.
- Ensure the CLI UIs can display:
  - A structured answer.
  - A citation list that matches the `[n]` references in the text.


6. Tool layer (tools/ folder) abstraction
-----------------------------------------
- Create a `tools/` package (or equivalent module) to encapsulate tool implementations:
  - `tools.single_page.py` – wraps single-page URL ingestion.
  - `tools.web_search.py` – wraps OpenAI `web_search` and structured output.
  - `tools.handbook.py` – wraps handbook loading/searching.
  - `tools.internal_search.py` – wraps `AcademicSearchEngine`.
- Each tool module should expose:
  - A Pydantic schema for tool input and output.
  - A callable that performs the work when the agent requests the tool.
  - A JSON schema/tool definition that can be passed to OpenAI’s tool/response API.
- Keep this layer generic so it can be reused by multiple agents or CLIs.


7. Search agent orchestration (multi-tool reasoning)
----------------------------------------------------
- Implement a higher-level “search agent” that:
  - Accepts a user query and optional configuration (e.g., allowed domains, tools_enabled).
  - Calls OpenAI (or a compatible provider) using:
    - A reasoning-capable model for complex, multi-step research flows.
    - A faster non-reasoning model for simpler queries when latency matters.
  - Has access to all tools:
    - Internal academic search.
    - Handbook.
    - Single web page fetch.
    - Broader web search with domain filters.
- Design the agent prompt/instructions so that it:
  - Decides when to call which tool(s).
  - Can chain tools (e.g., handbook first, then external web, then single-page fetch).
  - Always produces a final structured `AgentAnswer` with citations.
- Log or print tool usage in a developer-friendly way for debugging (e.g., “Agent is calling handbook tool…”).


8. Interactive terminal agent CLI
---------------------------------
- Add a dedicated CLI entrypoint, e.g. `search_agent_cli.py` or extend `ai_search.py`:
  - Provides an interactive REPL: user types questions, agent responds.
  - Maintains conversation history so the agent has context across turns.
  - Displays:
    - The answer text.
    - A nicely formatted citation list with `[n]` labels.
  - Optional verbose/debug mode to show which tools were used.
- Support configuration via CLI flags and environment variables:
  - Model selection (reasoning vs. non-reasoning).
  - API key overrides.
  - Enable/disable web search, handbook, internal index.
  - Allowed domains for web search.


9. Configuration and safety
---------------------------
- Centralize configuration for:
  - OpenAI model names (fast vs. reasoning variants).
  - Domain allowlists for web search.
  - Handbook path(s).
  - Maximum context sizes and truncation behavior (per tool).
- Add basic safety and robustness:
  - Timeouts and error handling around HTTP requests and OpenAI calls.
  - Clear fallback behavior if:
    - Web search is unavailable.
    - Handbook file is missing.
    - Internal index is empty.
  - Respect existing crawler ethics (robots.txt, rate limits) for any new crawling-like behavior.


10. Documentation and examples
------------------------------
- Update `README.md` to document the new agent features:
  - How to configure internal handbook(s).
  - How to enable web search and domain filters.
  - How to run the interactive search agent CLI.
  - Example queries showing:
    - Handbook-only answers.
    - Internal index answers.
    - Mixed internal + web answers with citations.
- Provide small, focused example scripts or snippets illustrating:
  - Calling the single-page tool directly.
  - Running a one-shot web search query.
  - Asking the unified agent a question and parsing `AgentAnswer`.


11. Multimodal document ingestion (PDF, Word, images)
-----------------------------------------------------
- Support user uploads of heterogeneous documents:
  - PDF papers, slide decks, Word/Office documents, markdown, text files.
  - Image-based content (scanned PDFs, figures, charts, gels, microscopy images).
- Implement a robust ingestion pipeline:
  - Use a layout-aware library (e.g., `docling`, `unstructured`, `pymupdf`, or `pdfplumber`) to:
    - Extract structured text blocks, headings, tables, figure captions, references.
    - Preserve logical sections (Abstract, Methods, Results, Discussion, etc.) when detectable.
  - For Word/Office files, use a converter (e.g., `python-docx` or a generic office-to-HTML/markdown pipeline).
  - For image-only PDFs or standalone images:
    - Use a vision-capable LLM or OCR backend (pluggable) to extract text and detect visual elements.
  - Normalize everything into an internal `Document` schema:
    - `id`, `source_type`, `file_name`, `mime_type`, `pages`, `sections`, `figures`, `tables`, `metadata`.
- Add a small ingestion CLI or API:
  - `ingest_docs.py` (or similar) that takes a folder of files and pushes them through the pipeline.
  - Store raw files plus parsed representations for reproducibility and re-indexing.


12. Vector database and embedding layer for RAG
----------------------------------------------
- Introduce a dedicated vector retrieval layer for uploaded documents:
  - Choose a pluggable vector store abstraction (e.g., local FAISS/SQLite, Chroma, Qdrant, Weaviate, or a managed service).
  - Design for multi-tenant / multi-corpus separation (e.g., per-project or per-user namespaces).
- Chunking strategy:
  - Split documents into semantically meaningful chunks:
    - Paragraphs, methodology steps, figure captions, table rows/blocks.
  - Include rich metadata in each chunk:
    - `doc_id`, `section`, `page`, `figure_id`, `method_step`, `doi`, `year`, `tags`.
  - Implement adaptive chunking (longer for narrative text, shorter for dense methods or formulas).
- Embedding strategy:
  - Use state-of-the-art text embedding models (configurable via settings), e.g.:
    - High-accuracy model for offline indexing.
    - Smaller/faster model for low-latency updates if needed.
  - For multimodal chunks (text + image reference):
    - Store both text embeddings and, where possible, image embeddings or image descriptors.
  - Support embedding re-indexing and schema versioning (to allow future model upgrades).
- Query-time retrieval:
  - Build a RAG retrieval function that:
    - Accepts a query and optional filters (document set, date range, tags, sections).
    - Retrieves top-k chunks from the vector store.
    - Optionally combines vector retrieval with keyword/TF-IDF filters for precision.
  - Integrate this as another tool for the search agent:
    - Input: query + constraints.
    - Output: ranked chunks + metadata + pre-formatted citations.


13. AlphaLab-style scientific integrity engine
---------------------------------------------
- Build a specialized pipeline for scientific papers inspired by the AlphaLab concept:

13.1 Visual entailment (PDF → figures → claims)
- For each paper:
  - Detect and extract figures/plots/charts + associated captions from PDFs.
  - Link each figure to nearby or referenced paragraphs in the text (using caption references like “Fig. 1”).
- Implement a visual entailment tool:
  - Uses a multimodal LLM (vision + text) to:
    - Read the claim paragraph(s).
    - Inspect the associated figure/plot/chart pixels.
    - Assess whether the visual evidence supports, contradicts, is unrelated to, or is insufficient for the claim.
  - Return a structured `VisualEntailmentResult`:
    - `claim_text`, `figure_id`, `verdict` (support/contradict/uncertain), `rationale`, and confidence.
- Integrate entailment into the RAG pipeline:
  - When answering questions about results or claims, optionally:
    - Run entailment checks in the background.
    - Surface warnings when claims are visually unsupported or ambiguous.

13.2 Methodology extraction to machine-readable protocols
- Extend the ingestion pipeline to identify Methods/Materials sections:
  - Detect section headers and subsections.
  - Segment into steps, parameters, and reagents.
- Use a structured extraction tool driven by a strong LLM:
  - Input: methods text.
  - Output: standardized JSON protocol:
    - `steps: List[Step]` where each `Step` has:
      - `description`, `temperature`, `time`, `reagent`, `equipment`, `volume`, `concentration`, `notes`.
    - `metadata`: organism/cell line, assay type, endpoints, etc.
- Store protocols alongside the vector index:
  - Index steps as separate chunks for RAG.
  - Allow direct querying of protocol steps (e.g., “What temperature is the incubation step?”).

13.3 Wet-lab debug mode
- Build a debug assistant specialized for experimental troubleshooting:
  - Input:
    - A structured protocol JSON (from the pipeline).
    - The user’s description of what they actually did (free text or structured).
    - Outcome description (e.g., “no bands on gel”, “low yield”, “high background”).
  - Use a reasoning model to:
    - Compare planned vs. executed steps.
    - Identify likely deviations (temperature, time, reagent lot, order of operations).
    - Suggest targeted hypotheses and corrective actions.
- Integrate with RAG:
  - Retrieve related protocols or troubleshooting guides from the vector store or web.
  - Provide grounded, cited suggestions rather than purely speculative advice.

13.4 Comparison mode across multiple papers
- Implement a comparison tool for cross-paper analysis:
  - Allow users to select or query multiple papers on a topic.
  - Retrieve and align:
    - Key results, effect sizes, confidence intervals.
    - Methodologies and critical parameters.
  - Use an LLM to:
    - Summarize consistencies and conflicts.
    - Highlight trends over time.
    - Flag methodological differences that may explain discrepancies.
- Expose this as:
  - A CLI command/flag (e.g., `--compare DOIs...` or named collections).
  - An agent tool that can be invoked when the question implies cross-paper synthesis.

13.5 Adversarial “reasoning console” (multi-agent debate)
- Implement a debate-style reasoning module for high-stakes judgments:
  - `Advocate` agent:
    - Argues in favor of the paper’s claims, citing evidence from text, figures, and external context.
  - `Skeptic` agent:
    - Actively hunts for flaws: underpowered samples, p-hacking, confounds, figure inconsistencies, missing controls.
  - `Judge` agent:
    - Synthesizes both sides.
    - Produces a structured verdict:
      - `verdict` (e.g., pass/caution/fail/inconclusive),
      - `key_issues`, `supporting_evidence`, `recommended_follow-ups`.
- Integrate with the search agent:
  - Expose the debate as a special tool for “integrity checks”.
  - Allow users to explicitly request a debate mode, or let the agent escalate to it for critical queries.


14. RAG orchestration and answer quality
----------------------------------------
- Design a unified RAG orchestrator used by the agent:
  - Plans which sources to query in which order:
    - Vector DB (uploaded docs).
    - Internal academic index.
    - Handbook.
    - Web search or single-page fetch.
  - Merges and re-ranks retrieved contexts (e.g., by relevance, recency, source trust).
- Implement answer composition strategies:
  - Use separate system instructions for:
    - Plain Q&A.
    - Integrity checks and adversarial analysis.
    - Protocol extraction and wet-lab debugging.
  - Encourage the model to:
    - Clearly label speculative vs. grounded statements.
    - Always reference citations `[n]` that map to concrete chunks or documents.
- Add optional answer verification:
  - Secondary “checker” call that:
    - Re-reads the answer + supporting chunks.
    - Flags unsupported claims or missing citations.
  - Optionally runs a lighter, automated sanity check (e.g., ensuring every factual claim has at least one citation).


15. Scalability, robustness, and system design
---------------------------------------------
- Architect ingestion and indexing as background jobs:
  - Use a task queue for large document batches.
  - Support incremental indexing and idempotent re-ingestion.
- Optimize vector search:
  - Use batching, caching, and approximate nearest neighbor (ANN) indexes where appropriate.
  - Shard or partition large corpora per organization/project.
- Observability:
  - Add logging and metrics for:
    - Tool calls (which tools are used, latency, error rates).
    - Retrieval quality signals (e.g., click-through, “answer helpful?” feedback).
  - Provide simple debug views that show:
    - Which chunks were retrieved.
    - Their scores and why they were chosen.
- Failure handling and resilience:
  - Graceful degradation when:
    - Vector DB is unavailable (fallback to TF-IDF + PageRank).
    - Web search fails (fallback to internal sources).
    - LLM calls error or time out (retry / fallback model).


16. Security, privacy, and access control
----------------------------------------
- Implement basic access control for uploaded documents:
  - Per-user or per-organization ownership.
  - Ensure vector indexes and raw files respect the same boundaries.
- Add mechanisms for:
  - Data deletion (user can remove documents and corresponding embeddings).
  - Redaction of sensitive fields before indexing (e.g., PII, PHI where required).
- Make external web search boundaries explicit:
  - Allow admins to configure which domains are permitted or blocked.
  - Clearly mark which answers depend on external vs. internal evidence.


17. Future extensions and research directions
--------------------------------------------
- Explore more advanced multimodal techniques:
  - Joint embeddings for text+image for better figure-based retrieval.
  - Automatic detection of statistical red flags (e.g., suspicious p-value patterns).
- Add benchmarking & evaluation:
  - Curate small test suites (questions + expected support) to track retrieval and answer quality over time.
  - Include integrity-focused benchmarks (detecting figure-text mismatches, missing controls, etc.).
- Provide optional UI/UX layers:
  - A simple web UI that:
    - Visualizes document structure, figures, protocols, and integrity verdicts.
    - Lets users drill down from answer → citation → original PDF region.


18. Asynchronous multi-file processing and specific file search
--------------------------------------------------------------
- Asynchronous ingestion:
  - Use a task queue system (e.g., Celery + Redis or an equivalent) to process uploads:
    - Handle multiple large PDFs/Word docs/images concurrently.
    - Avoid blocking the main API while parsing, chunking, and embedding documents.
  - Implement job tracking:
    - `upload_id` / `job_id` with statuses (pending, processing, completed, failed).
    - Simple endpoints or CLI commands to query ingestion status and errors.
- Multi-file organization:
  - Introduce a `Corpus` or `Collection` concept:
    - Group documents by project, experiment, or user.
    - Use corpus IDs/names for scoping search and RAG.
- Specific file and scoped search:
  - Allow queries that:
    - Target specific files or subsets (e.g., “only this PDF”, “these three reports”).
    - Filter by tags, document types, date ranges, or metadata (DOI, journal, etc.).
  - Propagate scope filters all the way through:
    - Vector store queries.
    - Traditional `AcademicSearchEngine` index queries (where applicable).
- UX/DevEx:
  - Provide clear feedback on which documents are included in each answer.
  - Enable “show only results from doc X” toggles both at the API level and in future UIs.


19. Intelligent query routing and summarisation strategy
-------------------------------------------------------
- Query classification:
  - Add a lightweight classifier (LLM- or rules-based) that labels queries as:
    - Overview / “What is this about?”
    - Specific detail / “What does Figure 3 show?”
    - Methodology / “How was this assay done?”
    - Debug / troubleshooting.
    - Cross-document comparison / literature review.
  - Use this label to drive routing decisions and tool selection.
- Routing patterns:
  - Overview queries:
    - Prefer pre-generated document or collection-level summaries (stored at ingestion time).
    - Use faster models when possible.
  - Specific queries:
    - Route through vector retrieval with smaller chunk sizes and parent-child context (see below).
  - Integrity/verification queries:
    - Invoke visual entailment and/or debate tools.
- Pre-generated summaries:
  - During ingestion, create:
    - Per-document short summary, long summary, and structured outline.
    - Optional section-level summaries (Abstract, Methods, Results, Discussion).
  - Store summaries in the DB and index them for faster overview responses.
- Gemini / OpenAI integration:
  - Keep the design model-agnostic but:
    - Allow configuration of which SDK/model is used for:
      - Summarisation.
      - Query classification.
      - Heavy reasoning (integrity checks, debates).


20. Hierarchical retrieval (parent/child) and query transformation (HyDe)
------------------------------------------------------------------------
- Hierarchical (parent/child) retrieval:
  - Chunk documents at two levels:
    - Parent chunks: larger sections (e.g., full paragraphs or section blocks).
    - Child chunks: fine-grained spans (e.g., 2–4 sentences, individual steps, caption-level blocks).
  - Index child chunks in the vector DB:
    - Retrieval is performed at child-level for high recall and precision.
  - At query time:
    - Retrieve top-k child chunks.
    - Map each child back to its parent chunk(s).
    - Provide the LLM with:
      - A compact set of parents (with child highlights) for richer context.
  - Expose configuration:
    - Number of child hits.
    - Max unique parents.
    - Deduplication logic for overlapping parents.
- Hybrid retrieval:
  - Combine vector retrieval with:
    - Keyword filters (BM25/TF-IDF).
    - Metadata filters (sections, figure IDs, methods).
  - Optionally use re-ranking models to reorder candidates before sending to the LLM.
- HyDe (Hypothetical Document Embeddings) query transformation:
  - Implement a HyDe module:
    - Step 1: Given a user query, ask a fast LLM to generate a short hypothetical answer or “ideal passage”.
    - Step 2: Embed this hypothetical passage instead of (or in addition to) the raw query.
    - Step 3: Use these embeddings for retrieval to bridge semantic gaps.
  - Allow configuration:
    - Whether to use HyDe always, never, or only for certain query types (e.g., complex, long, or low-recall queries).
  - Log and monitor:
    - When HyDe helps (e.g., higher precision/recall).
    - Edge cases where HyDe may mislead retrieval.


21. Production-ready service architecture and deployment
--------------------------------------------------------
- Backend service layer:
  - Implement a well-structured API service (e.g., FastAPI) to:
    - Handle document uploads and ingestion job creation.
    - Provide query endpoints:
      - Simple RAG QA.
      - Integrity checks.
      - Comparison mode.
      - Debug mode.
    - Expose management endpoints for:
      - Corpus/document listing.
      - Re-ingestion/re-indexing.
      - Health checks and metrics.
- Task processing:
  - Use Celery + Redis (or an equivalent durable queue + broker) for:
    - Ingestion, chunking, embedding, and integrity checks.
    - Computationally heavy workflows (multi-agent debates, cross-paper comparisons).
  - Design tasks to be:
    - Idempotent.
    - Composable (small tasks that can be chained).
- Persistent storage:
  - Use Postgres as the primary metadata store:
    - Users, corpora, documents, chunks, summaries, protocols, integrity verdicts.
    - Job/task metadata and audit logs as needed.
  - Integrate PGVector (or a similar extension) for:
    - Storing embeddings directly in Postgres when appropriate.
    - Keeping metadata and vectors closely coupled for simpler deployments.
  - Alternatively, support:
    - External vector DBs for large-scale deployments while keeping an abstraction layer.
- Frontend and UX:
  - Expose the core features through a frontend (e.g., React or a minimal web UI):
    - Upload documents and track ingestion progress.
    - Run queries with filters (corpus, document, timeframe).
    - Visualize citations, protocols, and integrity verdicts.
  - Provide clear explanations of:
    - Which tools were used for each answer.
    - How confident the system is and why.
- Containerization and deployment:
  - Provide Dockerfiles and/or docker-compose manifests that spin up:
    - API service.
    - Worker processes.
    - Postgres (+ PGVector) and Redis.
    - Optional vector DB instances if used.
  - Document deployment patterns:
    - Local development.
    - Single-node production.
    - Scaled deployments (horizontal API + worker scaling, separate DBs).

